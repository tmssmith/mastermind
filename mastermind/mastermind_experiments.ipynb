{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mastermind import Mastermind\n",
    "from q_learning import QLearning\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import combinations, combinations_with_replacement, product\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(episode_returns, title):\n",
    "    n_agents = episode_returns.shape[1]\n",
    "    fig,ax1 = plt.subplots(1,1)\n",
    "    fig.suptitle(title)\n",
    "    ax1.set_title('Mean undiscounted return of {} agents'.format(n_agents))\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Return')\n",
    "    ax1.grid()\n",
    "\n",
    "    ax1.plot(np.mean(episode_returns,axis=1),color='k');\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = Mastermind(3,4,10,None) # This works!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Mastermind(4,6,12,None)   # n_pegs, n_colours, n_rows\n",
    "codes = env.actions\n",
    "k = 20\n",
    "codes = random.sample(codes,k=k)\n",
    "env = Mastermind(4,6,12,codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(agent, env, n_episodes, codes=None, render = False):\n",
    "    test_returns = []\n",
    "    for _ in range(n_episodes):\n",
    "            if codes:\n",
    "                code = random.choice(codes)\n",
    "            else:\n",
    "                code = None\n",
    "            cumulative_reward=0\n",
    "            state = env.reset(code)\n",
    "            state_hash = state.tobytes()\n",
    "            if render:\n",
    "                env.render()\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                action = agent.policy(state_hash, test=True)\n",
    "                next_state, reward, terminal, _ = env.step(list(action))\n",
    "                if render:\n",
    "                    env.render()\n",
    "                next_state_hash = next_state.tobytes()\n",
    "                state = next_state\n",
    "                state_hash = next_state_hash\n",
    "                cumulative_reward+=reward\n",
    "            test_returns.append(cumulative_reward)\n",
    "    return test_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_control(env,gamma=0.9,eps=0.15,alpha=0.2,n_agents=20,n_episodes=150, n_tests = 1000, codes = None):\n",
    "    returns = np.zeros((n_episodes, n_agents))\n",
    "    test_returns = np.zeros((n_agents,n_tests))\n",
    "    for i in range(n_agents):\n",
    "        agent = QLearning(env,gamma,eps,alpha)        \n",
    "        for episode in range(n_episodes):\n",
    "            if codes:\n",
    "                code = random.choice(codes)\n",
    "            else:\n",
    "                code = None\n",
    "            cumulative_reward=0\n",
    "            state = env.reset(code)\n",
    "            state_hash = state.tobytes()\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                action = agent.policy(state_hash)\n",
    "                next_state, reward, terminal, _ = env.step(list(action))\n",
    "                next_state_hash = next_state.tobytes()\n",
    "                agent.update_q_table(state_hash, action, reward, next_state_hash)\n",
    "                state = next_state\n",
    "                state_hash = next_state_hash\n",
    "                cumulative_reward+=reward\n",
    "            returns[episode][i] = cumulative_reward\n",
    "        test_returns[i] = test_policy(agent,env,n_tests, False)\n",
    "    return returns, test_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns, test_performance = q_control(env=env,n_agents=20, alpha=0.1, eps=0.15, n_episodes = 100000, codes=None)\n",
    "plot_learning_curve(returns,f'Number of codes: {len(env.actions)}')\n",
    "print(f\"Average performance in evaluation: {np.mean(test_performance)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,code in enumerate(codes):\n",
    "#     agent, returns = q_control(env=env,n_agents=10, alpha=0.8, eps=0.1, n_episodes = 1000, codes = [codes[i]])\n",
    "#     plot_learning_curve(returns,f\"Codes: {codes[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,code in enumerate(codes):\n",
    "#     agent, returns = q_control(env=env,n_agents=10, alpha=0.8, n_episodes = 1000, codes = [codes[i],list(reversed(codes[i]))])\n",
    "#     plot_learning_curve(returns,f\"Codes: {codes[i]}, {list(reversed(codes[i]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_pairs = [list(code_pair) for code_pair in combinations(codes,2)]\n",
    "# print(code_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,code in enumerate(code_pairs):\n",
    "#     agent, returns = q_control(env=env,n_agents=10, alpha=0.8, eps=0.1, n_episodes = 1000, codes = code_pairs[i])\n",
    "#     plot_learning_curve(returns,f\"Codes: {code_pairs[i][0]}, {code_pairs[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_triples = [list(code_triple) for code_triple in combinations(codes,3)]\n",
    "\n",
    "# for i,code in enumerate(code_triples):\n",
    "#     agent, returns = q_control(env=env,n_agents=10, alpha=0.8, eps=0.1, n_episodes = 1000, codes = code_triples[i])\n",
    "#     plot_learning_curve(returns,f\"Codes: {code_triples[i][0]}, {code_triples[i][1]}, {code_triples[i][2]}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastermind-ojK2r-XO-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
